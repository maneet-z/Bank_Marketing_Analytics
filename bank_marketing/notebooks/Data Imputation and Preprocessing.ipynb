{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> As we have completed the Exploratory Data Analysis, we have obtained an overview on the specifics of each attributes in the dataset.\n",
    "Further on, we proceed to handle outliers and missing values in few attributes that were found from previous EDA.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic of Contents:\n",
    "* [Understanding Data](#first-bullet)\n",
    "* [Basic Data Analysis](#second-bullet)\n",
    "* [Effects of banking data on Term Deposit](#third-bullet)\n",
    "* [Effect of Campaign on Term Deposit](#four-bullet)\n",
    "* [Additional Attribute Effects](#five-bullet)\n",
    "* [Data Preprocessing](#six-bullet)\n",
    "* [Data Modelling](#seven-bullet)\n",
    "* [Model Analysis](#eight-bullet)\n",
    "* [Results](#nine-bullet)\n",
    "* [Future Leads to Marketing Campaigns](#ten-bullet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/MANEET/bank_marketing'\n",
    "data_add = '/data'\n",
    "report_add = '/report'\n",
    "figures_add = '/figures'\n",
    "experiment_add = '/experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + data_add +'./raw/bank-additional-full.csv', delimiter = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Outliers: In statistics outleirs or edge cases are defined values excludeing the range of 1.5 * Q3 of the sample. Based on the EDA from the previous steps \"Age\" and \"Campaign\" have outliers. But the both the variables are actually following the real world siutation and hence we cannot consider them as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the Occupation versus Age plot, it is evident that there are a few unknown and unemployed samples for higher age groups. Considering the data generated from Portugese Banking Institution, we assume that most clients belong to Portugese nationaltiy. The legal retirement age is 65 for employees in portugese and so we replace the age groups above 65 to retired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['job'][df['age']>65].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df[\"age\"]>65) & (df[\"job\"]=='unknown'), 'job'] = 'retired'\n",
    "df.loc[(df[\"age\"]>65) & (df[\"job\"]=='unemployed'), 'job'] = 'retired'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know the dataset is obtained from real world instances, we can generate relation between education and occupation. As there are high \"unknown\" instances in job and education, we can hypothesize education based on job and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_education_ct = pd.crosstab(df.job,df.education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_education_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_education_max = job_education_ct.idxmax(axis=1) #this stores key-value pair of job-education to predict education\n",
    "education_job_max = job_education_ct.idxmax(axis=0)#this stores key-value pair of educaiton-job to predict job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_job_max.pop('unknown')\n",
    "job_education_max.pop('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['education']=='unknown') & (df['job']=='unknown'),'job'] = random.choice(education_job_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in education_job_max.keys():\n",
    "    df.loc[(df['education'] =='unknown') & (df['job'] == i),'education'] = education_job_max[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in job_education_max.keys():\n",
    "    df.loc[(df['job'] =='unknown') & (df['education'] == i),'job'] = job_education_max[i]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After this operation, following replacements were conducted in the Education and Job fields.\n",
    "\n",
    "Occupation based on Education.\n",
    "\n",
    "-Management> High School.\n",
    "-Service > High School\n",
    "-House Maid > Basic\n",
    "\n",
    "-Education based on Occupation\n",
    "-basic 4y, 6y, 9y > blue collar\n",
    "-professional_course > technician"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pdays attribute has < 5% of the positive values and so we will drop them as an edge case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.drop('pdays',axis=1)\n",
    "df_copy.head() \n",
    "df = df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path + data_add + '/processed/data_imputation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing <a class=\"anchor\" id=\"six-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[:,df.columns !='y']\n",
    "target_df = df.loc[:,df.columns == 'y']\n",
    "train_df.to_csv(path + data_add + '/processed/Raw_sample_features.csv')\n",
    "target_df.to_csv(path + data_add + '/processed/Raw_sample_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cat_cols(df):\n",
    "    num_cols = list(df._get_numeric_data().columns)\n",
    "    cat_cols = list(set(df.columns) - set(num_cols))\n",
    "    print(\"Found {0} Numerical columns in DataFrame\".format(len(num_cols)))\n",
    "    print(\"Found {0} Categorical columns in DataFrame\".format(len(cat_cols)))\n",
    "    return num_cols, cat_cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features, categorical_features = find_cat_cols(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over,y_over = oversample.fit_resample(train_df,target_df)\n",
    "X_over.to_csv(path + data_add + '/processed/Over_sample_features.csv')\n",
    "y_over.to_csv(path + data_add + '/processed/Over_sample_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_under,y_under = undersample.fit_resample(train_df,target_df)\n",
    "X_over.to_csv(path + data_add + '/processed/Under_sample_features.csv')\n",
    "y_over.to_csv(path + data_add + '/processed/Under_sample_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Oversampled dataset shape %s' % len(X_over))\n",
    "print('Undersampled dataset shape %s' % len(X_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X,y):\n",
    "    numeric_features, categorical_features = find_cat_cols(X)\n",
    "    scaler = MinMaxScaler()\n",
    "    label_encoder = LabelEncoder()\n",
    "    X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "    y_preprocessed = pd.Series(label_encoder.fit_transform(y))\n",
    "    X_preprocessed = pd.concat([X,pd.get_dummies(X[categorical_features])],axis=1)\n",
    "    X_preprocessed.drop(labels = categorical_features,axis=1,inplace=True)\n",
    "    # df.to_csv(path + data_add + './processed/processed_data.csv')\n",
    "    return X_preprocessed, y_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over_preprocessed, y_over_preprocessed = preprocess_data(X_over,y_over)\n",
    "print(X_over_preprocessed.shape, y_over_preprocessed.shape)\n",
    "X_over_preprocessed.to_csv(path + data_add + '/processed/Over_processed_features.csv')\n",
    "y_over_preprocessed.to_csv(path + data_add + '/processed/Over_processed_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under_preprocessed,y_under_preprocessed = preprocess_data(X_under,y_under)\n",
    "print(X_under_preprocessed.shape, y_under_preprocessed.shape)\n",
    "X_under_preprocessed.to_csv(path + data_add + '/processed/Under_processed_features.csv')\n",
    "y_under_preprocessed.to_csv(path + data_add + '/processed/Under_processed_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_preprocessed,y_raw_preprocessed = preprocess_data(train_df,target_df)\n",
    "print(X_raw_preprocessed.shape,y_raw_preprocessed.shape)\n",
    "X_raw_preprocessed.to_csv(path + data_add + '/processed/Raw_processed_features.csv')\n",
    "y_raw_preprocessed.to_csv(path + data_add + '/processed/Raw_processed_labels.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
